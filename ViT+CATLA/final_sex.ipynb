{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98847e8e-1a49-4dd3-b48b-f23e3aa06f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\yolov7_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Anaconda\\envs\\yolov7_env\\lib\\site-packages\\torch\\cuda\\memory.py:278: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'F'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Anaconda\\envs\\yolov7_env\\lib\\site-packages\\sklearn\\utils\\_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\yolov7_env\\lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\yolov7_env\\lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\yolov7_env\\lib\\site-packages\\sklearn\\utils\\_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'F'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m num_sex_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sex_label_encoder\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# === Dataset Loaders ===\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mWildlifeDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfold_3/train/images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m WildlifeDataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASET_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold_3/val/images\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    117\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m WildlifeDataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASET_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/images\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m, in \u001b[0;36mWildlifeDataset.__init__\u001b[1;34m(self, root_dir)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths\u001b[38;5;241m.\u001b[39mappend(full_path)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mappend(\u001b[43msex_label_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msex_dir\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\yolov7_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\yolov7_env\\lib\\site-packages\\sklearn\\utils\\_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 'F'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, average_precision_score\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# === Reproducibility ===\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# === Dataset Path for SEX ===\n",
    "DATASET_PATH = r\"D:\\Master's Research\\unified_dataset\\split_sex\"\n",
    "\n",
    "# === Clear GPU Memory ===\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# === Load Metadata for SEX ===\n",
    "def load_metadata_dicts(excel_path):\n",
    "    xls = pd.ExcelFile(excel_path)\n",
    "    df1 = xls.parse('Fentonbury Apr_May 2024')\n",
    "    df2 = xls.parse('Bronte')\n",
    "    df1 = df1[['Individual_name', 'Sex', 'Age_years']]\n",
    "    df2 = df2[['Individual_name', 'Sex', 'Age_years']]\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    df['Individual_name'] = df['Individual_name'].astype(str).str.strip().str.lower()\n",
    "    df['Sex'] = df['Sex'].astype(str).str.strip().str.lower()\n",
    "\n",
    "    def map_age(age):\n",
    "        if pd.isna(age):\n",
    "            return None\n",
    "        elif age >= 4:\n",
    "            return 'more_than_4'\n",
    "        else:\n",
    "            return str(int(age))\n",
    "\n",
    "    df['Age_Group'] = df['Age_years'].apply(map_age)\n",
    "    df = df.dropna(subset=['Sex', 'Age_Group'])\n",
    "\n",
    "    sex_map = dict(zip(df['Individual_name'], df['Sex']))\n",
    "    age_map = dict(zip(df['Individual_name'], df['Age_Group']))\n",
    "    return sex_map, age_map\n",
    "\n",
    "# === Load Metadata and Encoders ===\n",
    "metadata_excel_path = r\"C:\\Users\\Jaylen LI\\Downloads\\1st_share\\1st_share\\All sites metadata for AI 2024_working copy.xlsx\"\n",
    "individual_to_sex, individual_to_age = load_metadata_dicts(metadata_excel_path)\n",
    "\n",
    "sex_label_encoder = LabelEncoder()\n",
    "sex_label_encoder.fit(list(set(individual_to_sex.values())))\n",
    "\n",
    "# === Dataset Class for SEX ===\n",
    "class WildlifeDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Expect structure: root_dir/sex/individual/image.jpg\n",
    "        for sex_dir in os.listdir(root_dir):\n",
    "            sex_path = os.path.join(root_dir, sex_dir)\n",
    "            if not os.path.isdir(sex_path):\n",
    "                continue\n",
    "\n",
    "            for indiv_name in os.listdir(sex_path):\n",
    "                indiv_path = os.path.join(sex_path, indiv_name)\n",
    "                if not os.path.isdir(indiv_path):\n",
    "                    continue\n",
    "\n",
    "                for img_name in os.listdir(indiv_path):\n",
    "                    full_path = os.path.join(indiv_path, img_name)\n",
    "                    if not os.path.isfile(full_path):\n",
    "                        continue\n",
    "                    if not img_name.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "                        continue\n",
    "\n",
    "                    self.image_paths.append(full_path)\n",
    "                    self.labels.append(sex_label_encoder.transform([sex_dir])[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        img_tensor = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return img_tensor, label, self.image_paths[idx]\n",
    "\n",
    "\n",
    "# === Shared Class Setup ===\n",
    "shared_class_names = sorted([\n",
    "    d.lower().strip() for d in os.listdir(os.path.join(DATASET_PATH, 'fold_3/train/images'))\n",
    "    if os.path.isdir(os.path.join(DATASET_PATH, 'fold_1/train/images', d))\n",
    "])\n",
    "num_sex_classes = len(sex_label_encoder.classes_)\n",
    "\n",
    "# === Dataset Loaders ===\n",
    "train_dataset = WildlifeDataset(os.path.join(DATASET_PATH, 'fold_3/train/images'))\n",
    "val_dataset = WildlifeDataset(os.path.join(DATASET_PATH, 'fold_3/val/images'))\n",
    "test_dataset = WildlifeDataset(os.path.join(DATASET_PATH, 'test/images'))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# === Visual Debug ===\n",
    "def show_example_images(dataset, num_examples=5):\n",
    "    fig, axes = plt.subplots(1, num_examples, figsize=(15, 5))\n",
    "    random_indices = random.sample(range(len(dataset)), num_examples)\n",
    "\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        image, label, img_path = dataset[idx]\n",
    "        title = f\"Class: {dataset.classes[label]}\"\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d5aff-11ee-48e2-8e44-48e6ad4cdb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Cross-Attention Block (CAB)\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.inner_patch_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.cross_patch_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Inner-Patch Self-Attention (IPSA)\n",
    "        x_residual = x\n",
    "        x = self.ln1(x)\n",
    "        x, _ = self.inner_patch_attention(x, x, x)\n",
    "        x = x_residual + x  # Skip connection\n",
    "\n",
    "        # Cross-Patch Self-Attention (CPSA)\n",
    "        x_residual = x\n",
    "        x = self.ln2(x)\n",
    "        x, _ = self.cross_patch_attention(x, x, x)\n",
    "        x = x_residual + x  # Skip connection\n",
    "        \n",
    "        # Feedforward MLP\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75861216-af56-437e-8bb2-f8536f8adddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement ViT Backbone with Cross-Attention\n",
    "class ViTBackbone(nn.Module):\n",
    "    def __init__(self, depth=3):\n",
    "        super().__init__()\n",
    "        self.vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "        self.attentions = []\n",
    "\n",
    "        # Register hook to capture attention maps\n",
    "        def get_attention_hook(module, input, output):\n",
    "            self.attentions.append(output)\n",
    "\n",
    "        # Hook into all attention layers\n",
    "        for blk in self.vit.blocks:\n",
    "            blk.attn.register_forward_hook(get_attention_hook)\n",
    "\n",
    "        self.cross_attention = nn.ModuleList([CrossAttentionBlock() for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.attentions = []  # Reset attention storage\n",
    "        x = self.vit.forward_features(x)\n",
    "        for cab in self.cross_attention:\n",
    "            x = cab(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27faee-9fa5-44c3-8a19-f61df35ef37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally Aware Network (LAN)\n",
    "class LocallyAwareNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_layers=7, lambda_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.lambda_weight = lambda_weight\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        global_token = x[:, 0]               # CLS token\n",
    "        local_tokens = x[:, 1:]              # Patch tokens\n",
    "        fused = (local_tokens + self.lambda_weight * global_token.unsqueeze(1)) / (1 + self.lambda_weight)\n",
    "        fused = fused.view(-1, self.num_layers, 28, 768)  # Reshape if needed\n",
    "        fused = self.fc(fused)\n",
    "        pooled = fused.mean(dim=2).mean(dim=1)\n",
    "        return pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac850b9b-b659-4578-b141-eceb674d56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full CATLA Transformer with Multi-task Heads\n",
    "class CATLATransformer(nn.Module):\n",
    "    def __init__(self, num_individuals, num_sex_classes, num_age_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = ViTBackbone(depth=3)  # From original CATLA paper\n",
    "        self.lan = LocallyAwareNetwork()\n",
    "        \n",
    "        # Multi-task heads\n",
    "        self.id_head = nn.Linear(768, num_individuals)\n",
    "        self.sex_head = nn.Linear(768, num_sex_classes)\n",
    "        self.age_head = nn.Linear(768, num_age_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        # x = self.cab(x)  ← REMOVE THIS LINE\n",
    "        x = self.lan(x)\n",
    "        sex_logits = self.sex_head(x)\n",
    "        return sex_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b987c9-13d6-4907-a1e9-3195ce0d19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CATLATransformer(\n",
    "    num_individuals=0,  # Not used for this task\n",
    "    num_sex_classes=len(sex_label_encoder.classes_),\n",
    "    num_age_classes=0   # Not used in this task\n",
    ").to(device)\n",
    "\n",
    "# Only use sex classification loss\n",
    "loss_fn_sex = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0b8d6-2261-4a78-8273-ed2b7f3eda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load partial checkpoint (ignore classifier mismatch)\n",
    "checkpoint = torch.load(\"best_CATLA_Transformer.pth\", map_location=device)\n",
    "\n",
    "# Get current model parameters\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# Filter out mismatched keys (like classifier weights/bias)\n",
    "filtered_dict = {k: v for k, v in checkpoint.items()\n",
    "                 if k in model_dict and model_dict[k].shape == v.shape}\n",
    "\n",
    "# Update model with compatible weights only\n",
    "model_dict.update(filtered_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "print(f\" Loaded {len(filtered_dict)} compatible parameters from checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9581dd-cd10-451b-9180-79c9416a04c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "num_epochs = 30\n",
    "patience = 5\n",
    "best_val_acc = 0\n",
    "counter = 0\n",
    "early_stop = False\n",
    "best_model_path = \"earlystop_CATLA_sex.pth\"\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs_sex, val_accs_sex = [], []\n",
    "\n",
    "true_train_sex, pred_train_sex = [], []\n",
    "true_val_sex, pred_val_sex = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct_sex = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, sex_labels, paths in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Train\"):\n",
    "        images = images.to(device)\n",
    "        sex_labels = sex_labels.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sex_logits = model(images)\n",
    "\n",
    "        true_train_sex.extend(sex_labels.cpu().numpy())\n",
    "        pred_train_sex.extend(sex_logits.argmax(1).cpu().numpy())\n",
    "\n",
    "        loss = ce_loss_fn(sex_logits, sex_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total += images.size(0)\n",
    "        correct_sex += (sex_logits.argmax(1) == sex_labels).sum().item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accs_sex.append(correct_sex / total)\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_sex = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, sex_labels, paths in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Val\"):\n",
    "            images = images.to(device)\n",
    "            sex_labels = sex_labels.to(device).long()\n",
    "\n",
    "            sex_logits = model(images)\n",
    "            loss = ce_loss_fn(sex_logits, sex_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            total += images.size(0)\n",
    "            correct_sex += (sex_logits.argmax(1) == sex_labels).sum().item()\n",
    "\n",
    "            true_val_sex.extend(sex_labels.cpu().numpy())\n",
    "            pred_val_sex.extend(sex_logits.argmax(1).cpu().numpy())\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accs_sex.append(correct_sex / total)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Sex Acc: {train_accs_sex[-1]:.4f} | Val Sex Acc: {val_accs_sex[-1]:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_accs_sex[-1] > best_val_acc:\n",
    "        best_val_acc = val_accs_sex[-1]\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "if early_stop:\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "else:\n",
    "    torch.save(model.state_dict(), \"final_CATLA_sex.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4db4a9-b337-4831-ab1c-5a1f13cee581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# === SEX REPORTS ===\n",
    "print(\"\\n Final SEX Classification Report (Train):\")\n",
    "print(classification_report(true_train_sex, pred_train_sex, target_names=sex_label_encoder.classes_))\n",
    "\n",
    "print(\"\\n Final SEX Classification Report (Validation):\")\n",
    "print(classification_report(true_val_sex, pred_val_sex, target_names=sex_label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf9fa2-17dd-4e78-b8b4-c1661e2180ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accs_sex, label=\"Train Sex Accuracy\")\n",
    "plt.plot(val_accs_sex, label=\"Val Sex Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Sex Classification Accuracy Over Epochs\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af2fb8-222a-48b5-bf63-4aecc784364d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.functional import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Ensure model is on correct device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# === Initialize Storage ===\n",
    "true_test_sex, pred_test_sex = [], []\n",
    "\n",
    "# === Run Inference on Test Set (sex Only) ===\n",
    "with torch.no_grad():\n",
    "    for images, sex_labels, paths in test_loader:\n",
    "        images = images.to(device)\n",
    "        sex_labels = sex_labels.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "\n",
    "        true_test_sex.extend(sex_labels.cpu().numpy())\n",
    "        pred_test_sex.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "# === Accuracy and F1 Score ===\n",
    "acc_sex = accuracy_score(true_test_sex, pred_test_sex)\n",
    "f1_sex = f1_score(true_test_sex, pred_test_sex, average=\"weighted\")\n",
    "\n",
    "print(\"\\n Sex Classification Metrics (Test):\")\n",
    "print(f\"Accuracy: {acc_sex:.4f} | F1 Score: {f1_sex:.4f}\")\n",
    "\n",
    "# === Handle Label Mismatch Safely ===\n",
    "unique_labels = np.unique(true_test_sex)\n",
    "target_names = sex_label_encoder.inverse_transform(unique_labels)\n",
    "\n",
    "print(\"\\n Classification Report (Sex):\")\n",
    "print(classification_report(true_test_sex, pred_test_sex, labels=unique_labels, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f1e25-0172-4a01-92a1-4ee55df425c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def save_classification_report(true_labels, pred_labels, label_encoder, file_path):\n",
    "    \"\"\"\n",
    "    Save classification report to CSV with only valid target_names (present in labels).\n",
    "    \"\"\"\n",
    "    labels_present = np.unique(true_labels)\n",
    "    target_names_present = label_encoder.inverse_transform(labels_present)\n",
    "    \n",
    "    report_dict = classification_report(\n",
    "        true_labels,\n",
    "        pred_labels,\n",
    "        labels=labels_present,\n",
    "        target_names=target_names_present,\n",
    "        output_dict=True\n",
    "    )\n",
    "    df = pd.DataFrame(report_dict).transpose()\n",
    "    df.to_csv(file_path)\n",
    "    print(f\" Classification report saved to: {file_path}\")\n",
    "\n",
    "output_dir = r\"D:\\Master's Research\\ViT+CATLA Transformer\\results_sex\"\n",
    "\n",
    "save_classification_report(true_train_sex, pred_train_sex, sex_label_encoder,\n",
    "    os.path.join(output_dir, \"fold_report_train_sex.csv\"))\n",
    "\n",
    "save_classification_report(true_val_sex, pred_val_sex, sex_label_encoder,\n",
    "    os.path.join(output_dir, \"fold_report_val_sex.csv\"))\n",
    "\n",
    "save_classification_report(true_test_sex, pred_test_sex, sex_label_encoder,\n",
    "    os.path.join(output_dir, \"fold_report_test_sex.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd07490-f6a1-42a2-bb8f-8acc29b7dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(true_labels, pred_labels, class_names,\n",
    "                          dataset_name=\"Confusion Matrix\", file_path=None):\n",
    "    labels = list(range(len(class_names)))\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=labels)\n",
    "\n",
    "    # Normalize row-wise\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    # === Save to CSV ===\n",
    "    if file_path:\n",
    "        df_cm = pd.DataFrame(cm_norm, index=class_names, columns=class_names)\n",
    "        df_cm.to_csv(file_path)\n",
    "        print(f\"✅ Saved: {file_path}\")\n",
    "\n",
    "    # === Plot ===\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                square=True, cbar_kws={\"label\": \"Proportion (0–1)\"})\n",
    "    plt.title(dataset_name)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === Paths ===\n",
    "output_dir = r\"D:\\Master's Research\\ViT+CATLA Transformer\\results_sex\"\n",
    "\n",
    "# === Test Set Plot ===\n",
    "plot_confusion_matrix(\n",
    "    true_test_sex,\n",
    "    pred_test_sex,\n",
    "    sex_label_encoder.classes_,\n",
    "    dataset_name=\"Test Confusion Matrix - Sex\",\n",
    "    file_path=os.path.join(output_dir, \"fold5_cm_test_sex.csv\")\n",
    ")\n",
    "\n",
    "# === Validation Set Plot ===\n",
    "plot_confusion_matrix(\n",
    "    true_val_sex,\n",
    "    pred_val_sex,\n",
    "    sex_label_encoder.classes_,\n",
    "    dataset_name=\"Validation Confusion Matrix - Sex\",\n",
    "    file_path=os.path.join(output_dir, \"fold5_cm_val_sex.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f873508-6831-4d62-80c8-05df5827c23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax, sigmoid\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def plot_multiclass_roc(true_labels, pred_probs, class_names, title=\"ROC Curve\", save_path=None, auc_csv_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curves and export AUC scores to CSV.\n",
    "    Handles both binary and multi-class settings.\n",
    "    \"\"\"\n",
    "    num_classes = pred_probs.shape[1]\n",
    "    \n",
    "    # 🛠 Ensure label_binarize outputs shape (N, num_classes)\n",
    "    unique_classes = sorted(np.unique(true_labels))\n",
    "    true_bin = label_binarize(true_labels, classes=list(range(num_classes)))\n",
    "\n",
    "    if true_bin.shape[1] == 1 and num_classes == 2:\n",
    "        # Convert (502, 1) → (502, 2) for binary classification\n",
    "        true_bin = np.hstack([1 - true_bin, true_bin])\n",
    "\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        if i >= pred_probs.shape[1]:\n",
    "            print(f\" Skipping class {i} due to insufficient prediction dimensions.\")\n",
    "            continue\n",
    "\n",
    "        if np.sum(true_bin[:, i]) == 0:\n",
    "            print(f\" Skipping class {i} due to no positive samples.\")\n",
    "            continue\n",
    "\n",
    "        y_score = pred_probs[:, i]\n",
    "        fpr[i], tpr[i], _ = roc_curve(true_bin[:, i], y_score)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        label = class_names[i] if i < len(class_names) else f\"Class {i}\"\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, label=f'{label} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(\n",
    "        loc='center left',\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        borderaxespad=0.,\n",
    "        fontsize='small'\n",
    "    )\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\" ROC curve saved to: {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    if auc_csv_path:\n",
    "        auc_data = {class_names[i]: [roc_auc[i]] for i in roc_auc}\n",
    "        auc_df = pd.DataFrame(auc_data).T\n",
    "        auc_df.columns = [\"AUC\"]\n",
    "        os.makedirs(os.path.dirname(auc_csv_path), exist_ok=True)\n",
    "        auc_df.to_csv(auc_csv_path)\n",
    "        print(f\" AUC scores saved to: {auc_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e48bf-ba0f-43f8-bddd-bb5129762ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_test_sex, pred_test_sex = [], []\n",
    "sex_logits_test = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, paths in tqdm(test_loader, desc=\"Test Inference – Sex\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "\n",
    "        # True + predicted labels\n",
    "        true_test_sex.extend(labels.cpu().numpy())\n",
    "        pred_test_sex.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        # For ROC\n",
    "        sex_logits_test.append(logits.cpu())\n",
    "\n",
    "# Stack logits for all batches\n",
    "sex_logits_test = torch.cat(sex_logits_test, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a71b09-99d1-4dba-964b-09fe3cc7fd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Apply softmax to get [P(class 0), P(class 1)] for each image\n",
    "sex_probs_test = softmax(sex_logits_test, dim=1).cpu().numpy()  # shape: (N, 2)\n",
    "\n",
    "# Optional: print shapes to confirm\n",
    "print(\"true_test_sex:\", len(true_test_sex))\n",
    "print(\"sex_logits_test:\", sex_logits_test.shape)\n",
    "print(\"sex_probs_test:\", sex_probs_test.shape)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plot_multiclass_roc(\n",
    "    true_test_sex,\n",
    "    sex_probs_test,\n",
    "    sex_label_encoder.classes_,\n",
    "    title=\"Test ROC – Sex\",\n",
    "    save_path=r\"D:\\Master's Research\\ViT+CATLA Transformer\\results_sex\\roc_test_sex.png\",\n",
    "    auc_csv_path=r\"D:\\Master's Research\\ViT+CATLA Transformer\\results_sex\\roc_test_sex_auc.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195a684-18f4-4a24-b6ff-1356cd20da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Root path to analyze\n",
    "dataset_root = r\"D:\\Master's Research\\unified_dataset\\final_split_safe\"\n",
    "\n",
    "# Results\n",
    "summary = []\n",
    "resolution_data = []\n",
    "\n",
    "# Walk through all fold_X/train|val/images and test/images\n",
    "for root, dirs, files in os.walk(dataset_root):\n",
    "    if \"images\" not in root:\n",
    "        continue\n",
    "\n",
    "    for class_dir in dirs:\n",
    "        class_path = os.path.join(root, class_dir)\n",
    "        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        # Extract split and set\n",
    "        path_parts = os.path.normpath(class_path).split(os.sep)\n",
    "        if 'test' in path_parts:\n",
    "            split = 'test'\n",
    "            set_type = 'test'\n",
    "        else:\n",
    "            split = path_parts[-4]  # e.g., fold_1\n",
    "            set_type = path_parts[-3]  # train or val\n",
    "\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    width, height = img.size\n",
    "                    resolution_data.append({\n",
    "                        \"Split\": split,\n",
    "                        \"Set\": set_type,\n",
    "                        \"Class\": class_dir,\n",
    "                        \"Image\": img_file,\n",
    "                        \"Width\": width,\n",
    "                        \"Height\": height\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error reading {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        summary.append({\n",
    "            \"Split\": split,\n",
    "            \"Set\": set_type,\n",
    "            \"Class\": class_dir,\n",
    "            \"Num_Images\": len(image_files)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_res = pd.DataFrame(resolution_data)\n",
    "\n",
    "# Pivot for image count table\n",
    "pivot_summary = df_summary.pivot_table(index=['Split', 'Set'], columns='Class', values='Num_Images', aggfunc='sum').fillna(0).astype(int)\n",
    "\n",
    "# Resolution stats\n",
    "res_stats = df_res.groupby(['Split', 'Set', 'Class'])[['Width', 'Height']].agg(['min', 'max', 'mean']).round(1)\n",
    "\n",
    "# Save to Excel\n",
    "output_file = os.path.join(dataset_root, \"dataset_analysis.xlsx\")\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_summary.to_excel(writer, sheet_name=\"Image_Counts\", index=False)\n",
    "    df_res.to_excel(writer, sheet_name=\"Resolutions\", index=False)\n",
    "    res_stats.to_excel(writer, sheet_name=\"Resolution_Stats\")\n",
    "\n",
    "print(f\" Dataset analysis saved to: {output_file}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487c3ac-78b2-419b-8a89-9d848aa14ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolov7_env)",
   "language": "python",
   "name": "yolov7_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
